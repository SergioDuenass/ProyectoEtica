2024-04-04 12:40:32,818:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-04 12:40:32,818:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-04 12:40:32,818:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-04 12:40:32,818:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-04 12:40:33,015:INFO:PyCaret ClassificationExperiment
2024-04-04 12:40:33,015:INFO:Logging name: clf-default-name
2024-04-04 12:40:33,015:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-04-04 12:40:33,015:INFO:version 3.3.0
2024-04-04 12:40:33,015:INFO:Initializing setup()
2024-04-04 12:40:33,015:INFO:self.USI: eb4d
2024-04-04 12:40:33,015:INFO:self._variable_keys: {'fix_imbalance', 'seed', 'logging_param', 'log_plots_param', 'exp_name_log', 'X_test', 'y_train', 'idx', 'gpu_n_jobs_param', '_ml_usecase', 'fold_shuffle_param', 'is_multiclass', 'data', 'X_train', 'n_jobs_param', 'gpu_param', 'fold_generator', 'fold_groups_param', 'pipeline', 'exp_id', 'memory', 'USI', 'y', 'target_param', 'html_param', 'X', 'y_test', '_available_plots'}
2024-04-04 12:40:33,015:INFO:Checking environment
2024-04-04 12:40:33,015:INFO:python_version: 3.9.19
2024-04-04 12:40:33,015:INFO:python_build: ('main', 'Mar 21 2024 12:07:41')
2024-04-04 12:40:33,015:INFO:machine: arm64
2024-04-04 12:40:33,015:INFO:platform: macOS-14.2-arm64-arm-64bit
2024-04-04 12:40:33,015:INFO:Memory: svmem(total=17179869184, available=4680187904, percent=72.8, used=7149830144, free=83116032, active=4607295488, inactive=4540579840, wired=2542534656)
2024-04-04 12:40:33,015:INFO:Physical Core: 10
2024-04-04 12:40:33,015:INFO:Logical Core: 10
2024-04-04 12:40:33,015:INFO:Checking libraries
2024-04-04 12:40:33,015:INFO:System:
2024-04-04 12:40:33,015:INFO:    python: 3.9.19 (main, Mar 21 2024, 12:07:41)  [Clang 14.0.6 ]
2024-04-04 12:40:33,015:INFO:executable: /opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/bin/python
2024-04-04 12:40:33,015:INFO:   machine: macOS-14.2-arm64-arm-64bit
2024-04-04 12:40:33,015:INFO:PyCaret required dependencies:
2024-04-04 12:40:33,031:INFO:                 pip: 23.3.1
2024-04-04 12:40:33,031:INFO:          setuptools: 68.2.2
2024-04-04 12:40:33,031:INFO:             pycaret: 3.3.0
2024-04-04 12:40:33,031:INFO:             IPython: 8.12.0
2024-04-04 12:40:33,031:INFO:          ipywidgets: 8.1.2
2024-04-04 12:40:33,031:INFO:                tqdm: 4.66.2
2024-04-04 12:40:33,031:INFO:               numpy: 1.26.4
2024-04-04 12:40:33,031:INFO:              pandas: 2.1.4
2024-04-04 12:40:33,031:INFO:              jinja2: 3.1.3
2024-04-04 12:40:33,031:INFO:               scipy: 1.11.4
2024-04-04 12:40:33,031:INFO:              joblib: 1.3.2
2024-04-04 12:40:33,031:INFO:             sklearn: 1.4.1.post1
2024-04-04 12:40:33,031:INFO:                pyod: 1.1.3
2024-04-04 12:40:33,031:INFO:            imblearn: 0.12.2
2024-04-04 12:40:33,031:INFO:   category_encoders: 2.6.3
2024-04-04 12:40:33,031:INFO:            lightgbm: 4.3.0
2024-04-04 12:40:33,031:INFO:               numba: 0.59.1
2024-04-04 12:40:33,031:INFO:            requests: 2.31.0
2024-04-04 12:40:33,031:INFO:          matplotlib: 3.7.5
2024-04-04 12:40:33,031:INFO:          scikitplot: 0.3.7
2024-04-04 12:40:33,031:INFO:         yellowbrick: 1.5
2024-04-04 12:40:33,031:INFO:              plotly: 5.20.0
2024-04-04 12:40:33,031:INFO:    plotly-resampler: Not installed
2024-04-04 12:40:33,031:INFO:             kaleido: 0.2.1
2024-04-04 12:40:33,031:INFO:           schemdraw: 0.15
2024-04-04 12:40:33,031:INFO:         statsmodels: 0.14.1
2024-04-04 12:40:33,031:INFO:              sktime: 0.28.0
2024-04-04 12:40:33,031:INFO:               tbats: 1.1.3
2024-04-04 12:40:33,031:INFO:            pmdarima: 2.0.4
2024-04-04 12:40:33,031:INFO:              psutil: 5.9.8
2024-04-04 12:40:33,031:INFO:          markupsafe: 2.1.5
2024-04-04 12:40:33,031:INFO:             pickle5: Not installed
2024-04-04 12:40:33,031:INFO:         cloudpickle: 3.0.0
2024-04-04 12:40:33,031:INFO:         deprecation: 2.1.0
2024-04-04 12:40:33,031:INFO:              xxhash: 3.4.1
2024-04-04 12:40:33,031:INFO:           wurlitzer: 3.0.3
2024-04-04 12:40:33,031:INFO:PyCaret optional dependencies:
2024-04-04 12:40:33,036:INFO:                shap: Not installed
2024-04-04 12:40:33,036:INFO:           interpret: Not installed
2024-04-04 12:40:33,036:INFO:                umap: Not installed
2024-04-04 12:40:33,036:INFO:     ydata_profiling: Not installed
2024-04-04 12:40:33,036:INFO:  explainerdashboard: Not installed
2024-04-04 12:40:33,036:INFO:             autoviz: Not installed
2024-04-04 12:40:33,036:INFO:           fairlearn: Not installed
2024-04-04 12:40:33,037:INFO:          deepchecks: Not installed
2024-04-04 12:40:33,037:INFO:             xgboost: Not installed
2024-04-04 12:40:33,037:INFO:            catboost: Not installed
2024-04-04 12:40:33,037:INFO:              kmodes: Not installed
2024-04-04 12:40:33,037:INFO:             mlxtend: Not installed
2024-04-04 12:40:33,037:INFO:       statsforecast: Not installed
2024-04-04 12:40:33,037:INFO:        tune_sklearn: Not installed
2024-04-04 12:40:33,037:INFO:                 ray: Not installed
2024-04-04 12:40:33,037:INFO:            hyperopt: Not installed
2024-04-04 12:40:33,037:INFO:              optuna: Not installed
2024-04-04 12:40:33,037:INFO:               skopt: Not installed
2024-04-04 12:40:33,037:INFO:              mlflow: Not installed
2024-04-04 12:40:33,037:INFO:              gradio: Not installed
2024-04-04 12:40:33,037:INFO:             fastapi: Not installed
2024-04-04 12:40:33,037:INFO:             uvicorn: Not installed
2024-04-04 12:40:33,037:INFO:              m2cgen: Not installed
2024-04-04 12:40:33,037:INFO:           evidently: Not installed
2024-04-04 12:40:33,037:INFO:               fugue: Not installed
2024-04-04 12:40:33,037:INFO:           streamlit: Not installed
2024-04-04 12:40:33,037:INFO:             prophet: Not installed
2024-04-04 12:40:33,037:INFO:None
2024-04-04 12:40:33,037:INFO:Set up data.
2024-04-04 12:40:33,039:INFO:Set up folding strategy.
2024-04-04 12:40:33,039:INFO:Set up train/test split.
2024-04-04 12:40:33,042:INFO:Set up index.
2024-04-04 12:40:33,042:INFO:Assigning column types.
2024-04-04 12:40:33,043:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-04-04 12:40:33,059:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-04-04 12:40:33,061:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-04 12:40:33,074:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,091:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-04-04 12:40:33,091:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-04 12:40:33,101:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,101:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,102:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-04-04 12:40:33,118:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-04 12:40:33,128:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,129:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,145:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-04 12:40:33,155:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,155:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,155:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-04-04 12:40:33,182:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,182:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,210:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,210:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,212:INFO:Preparing preprocessing pipeline...
2024-04-04 12:40:33,213:INFO:Set up simple imputation.
2024-04-04 12:40:33,214:INFO:Set up encoding of ordinal features.
2024-04-04 12:40:33,215:INFO:Set up encoding of categorical features.
2024-04-04 12:40:33,215:INFO:Set up column name cleaning.
2024-04-04 12:40:33,243:INFO:Finished creating preprocessing pipeline.
2024-04-04 12:40:33,251:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/vj/xp36g_9s7lj2v05vvcsms4500000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Income', 'CCAvg', 'Mortgage'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',...
                 TransformerWrapper(exclude=None, include=['Education'],
                                    transformer=OneHotEncoder(cols=['Education'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-04-04 12:40:33,251:INFO:Creating final display dataframe.
2024-04-04 12:40:33,313:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target     Personal.Loan
2                   Target type            Binary
3           Original data shape         (5000, 6)
4        Transformed data shape         (5000, 8)
5   Transformed train set shape         (3500, 8)
6    Transformed test set shape         (1500, 8)
7              Numeric features                 3
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              eb4d
2024-04-04 12:40:33,344:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,344:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,375:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,375:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-04 12:40:33,376:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:51: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.
  warnings.warn(

2024-04-04 12:40:33,377:INFO:setup() successfully completed in 0.36s...............
2024-04-04 12:40:33,381:INFO:Initializing compare_models()
2024-04-04 12:40:33,381:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-04-04 12:40:33,381:INFO:Checking exceptions
2024-04-04 12:40:33,384:INFO:Preparing display monitor
2024-04-04 12:40:33,405:INFO:Initializing Logistic Regression
2024-04-04 12:40:33,405:INFO:Total runtime is 3.612041473388672e-06 minutes
2024-04-04 12:40:33,406:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:33,406:INFO:Initializing create_model()
2024-04-04 12:40:33,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:33,406:INFO:Checking exceptions
2024-04-04 12:40:33,406:INFO:Importing libraries
2024-04-04 12:40:33,407:INFO:Copying training dataset
2024-04-04 12:40:33,410:INFO:Defining folds
2024-04-04 12:40:33,410:INFO:Declaring metric variables
2024-04-04 12:40:33,411:INFO:Importing untrained model
2024-04-04 12:40:33,413:INFO:Logistic Regression Imported successfully
2024-04-04 12:40:33,415:INFO:Starting cross validation
2024-04-04 12:40:33,416:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:35,059:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,170:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,180:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,181:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,201:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,203:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,228:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,260:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,261:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,262:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,279:INFO:Calculating mean and std
2024-04-04 12:40:35,282:INFO:Creating metrics dataframe
2024-04-04 12:40:35,288:INFO:Uploading results into container
2024-04-04 12:40:35,289:INFO:Uploading model into container now
2024-04-04 12:40:35,290:INFO:_master_model_container: 1
2024-04-04 12:40:35,290:INFO:_display_container: 2
2024-04-04 12:40:35,291:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-04-04 12:40:35,291:INFO:create_model() successfully completed......................................
2024-04-04 12:40:35,365:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:35,365:INFO:Creating metrics dataframe
2024-04-04 12:40:35,369:INFO:Initializing K Neighbors Classifier
2024-04-04 12:40:35,369:INFO:Total runtime is 0.03274276653925578 minutes
2024-04-04 12:40:35,371:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:35,371:INFO:Initializing create_model()
2024-04-04 12:40:35,371:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:35,371:INFO:Checking exceptions
2024-04-04 12:40:35,371:INFO:Importing libraries
2024-04-04 12:40:35,371:INFO:Copying training dataset
2024-04-04 12:40:35,374:INFO:Defining folds
2024-04-04 12:40:35,374:INFO:Declaring metric variables
2024-04-04 12:40:35,375:INFO:Importing untrained model
2024-04-04 12:40:35,377:INFO:K Neighbors Classifier Imported successfully
2024-04-04 12:40:35,381:INFO:Starting cross validation
2024-04-04 12:40:35,381:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:35,466:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,467:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,468:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,469:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,471:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,474:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,474:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,479:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,492:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,495:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,507:INFO:Calculating mean and std
2024-04-04 12:40:35,508:INFO:Creating metrics dataframe
2024-04-04 12:40:35,509:INFO:Uploading results into container
2024-04-04 12:40:35,509:INFO:Uploading model into container now
2024-04-04 12:40:35,510:INFO:_master_model_container: 2
2024-04-04 12:40:35,510:INFO:_display_container: 2
2024-04-04 12:40:35,510:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-04-04 12:40:35,510:INFO:create_model() successfully completed......................................
2024-04-04 12:40:35,560:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:35,560:INFO:Creating metrics dataframe
2024-04-04 12:40:35,563:INFO:Initializing Naive Bayes
2024-04-04 12:40:35,563:INFO:Total runtime is 0.03597924709320068 minutes
2024-04-04 12:40:35,565:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:35,565:INFO:Initializing create_model()
2024-04-04 12:40:35,565:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:35,565:INFO:Checking exceptions
2024-04-04 12:40:35,565:INFO:Importing libraries
2024-04-04 12:40:35,565:INFO:Copying training dataset
2024-04-04 12:40:35,567:INFO:Defining folds
2024-04-04 12:40:35,567:INFO:Declaring metric variables
2024-04-04 12:40:35,569:INFO:Importing untrained model
2024-04-04 12:40:35,570:INFO:Naive Bayes Imported successfully
2024-04-04 12:40:35,572:INFO:Starting cross validation
2024-04-04 12:40:35,573:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:35,607:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,610:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,619:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,619:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,621:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,623:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,625:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,633:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,633:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,637:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,653:INFO:Calculating mean and std
2024-04-04 12:40:35,653:INFO:Creating metrics dataframe
2024-04-04 12:40:35,654:INFO:Uploading results into container
2024-04-04 12:40:35,654:INFO:Uploading model into container now
2024-04-04 12:40:35,655:INFO:_master_model_container: 3
2024-04-04 12:40:35,655:INFO:_display_container: 2
2024-04-04 12:40:35,655:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-04-04 12:40:35,655:INFO:create_model() successfully completed......................................
2024-04-04 12:40:35,705:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:35,705:INFO:Creating metrics dataframe
2024-04-04 12:40:35,709:INFO:Initializing Decision Tree Classifier
2024-04-04 12:40:35,709:INFO:Total runtime is 0.03840570052464803 minutes
2024-04-04 12:40:35,711:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:35,711:INFO:Initializing create_model()
2024-04-04 12:40:35,711:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:35,711:INFO:Checking exceptions
2024-04-04 12:40:35,711:INFO:Importing libraries
2024-04-04 12:40:35,711:INFO:Copying training dataset
2024-04-04 12:40:35,714:INFO:Defining folds
2024-04-04 12:40:35,714:INFO:Declaring metric variables
2024-04-04 12:40:35,716:INFO:Importing untrained model
2024-04-04 12:40:35,717:INFO:Decision Tree Classifier Imported successfully
2024-04-04 12:40:35,721:INFO:Starting cross validation
2024-04-04 12:40:35,722:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:35,765:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,765:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,769:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,771:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,778:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,784:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,787:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,790:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,801:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,807:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:35,815:INFO:Calculating mean and std
2024-04-04 12:40:35,816:INFO:Creating metrics dataframe
2024-04-04 12:40:35,817:INFO:Uploading results into container
2024-04-04 12:40:35,817:INFO:Uploading model into container now
2024-04-04 12:40:35,817:INFO:_master_model_container: 4
2024-04-04 12:40:35,817:INFO:_display_container: 2
2024-04-04 12:40:35,818:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-04-04 12:40:35,818:INFO:create_model() successfully completed......................................
2024-04-04 12:40:35,866:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:35,866:INFO:Creating metrics dataframe
2024-04-04 12:40:35,869:INFO:Initializing SVM - Linear Kernel
2024-04-04 12:40:35,869:INFO:Total runtime is 0.04108202854792277 minutes
2024-04-04 12:40:35,871:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:35,871:INFO:Initializing create_model()
2024-04-04 12:40:35,871:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:35,871:INFO:Checking exceptions
2024-04-04 12:40:35,871:INFO:Importing libraries
2024-04-04 12:40:35,871:INFO:Copying training dataset
2024-04-04 12:40:35,873:INFO:Defining folds
2024-04-04 12:40:35,874:INFO:Declaring metric variables
2024-04-04 12:40:35,875:INFO:Importing untrained model
2024-04-04 12:40:35,876:INFO:SVM - Linear Kernel Imported successfully
2024-04-04 12:40:35,879:INFO:Starting cross validation
2024-04-04 12:40:35,879:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:35,915:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:35,921:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:35,922:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:35,924:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:35,930:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:35,930:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:35,932:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:35,939:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:35,941:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:35,954:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:35,954:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:35,956:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:35,970:INFO:Calculating mean and std
2024-04-04 12:40:35,971:INFO:Creating metrics dataframe
2024-04-04 12:40:35,972:INFO:Uploading results into container
2024-04-04 12:40:35,972:INFO:Uploading model into container now
2024-04-04 12:40:35,972:INFO:_master_model_container: 5
2024-04-04 12:40:35,972:INFO:_display_container: 2
2024-04-04 12:40:35,973:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-04-04 12:40:35,973:INFO:create_model() successfully completed......................................
2024-04-04 12:40:36,019:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:36,020:INFO:Creating metrics dataframe
2024-04-04 12:40:36,023:INFO:Initializing Ridge Classifier
2024-04-04 12:40:36,023:INFO:Total runtime is 0.043646331628163657 minutes
2024-04-04 12:40:36,025:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:36,025:INFO:Initializing create_model()
2024-04-04 12:40:36,025:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:36,025:INFO:Checking exceptions
2024-04-04 12:40:36,025:INFO:Importing libraries
2024-04-04 12:40:36,025:INFO:Copying training dataset
2024-04-04 12:40:36,028:INFO:Defining folds
2024-04-04 12:40:36,028:INFO:Declaring metric variables
2024-04-04 12:40:36,029:INFO:Importing untrained model
2024-04-04 12:40:36,031:INFO:Ridge Classifier Imported successfully
2024-04-04 12:40:36,034:INFO:Starting cross validation
2024-04-04 12:40:36,036:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:36,081:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:36,083:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:36,084:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:36,085:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:36,085:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:36,092:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:36,101:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:36,101:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:36,123:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:36,124:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 344, in _score
    response_method = _check_response_method(estimator, self._response_method)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/validation.py", line 2106, in _check_response_method
    raise AttributeError(
AttributeError: Pipeline has none of the following attributes: predict_proba.

  warnings.warn(

2024-04-04 12:40:36,140:INFO:Calculating mean and std
2024-04-04 12:40:36,140:INFO:Creating metrics dataframe
2024-04-04 12:40:36,142:INFO:Uploading results into container
2024-04-04 12:40:36,142:INFO:Uploading model into container now
2024-04-04 12:40:36,142:INFO:_master_model_container: 6
2024-04-04 12:40:36,142:INFO:_display_container: 2
2024-04-04 12:40:36,142:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-04-04 12:40:36,142:INFO:create_model() successfully completed......................................
2024-04-04 12:40:36,193:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:36,193:INFO:Creating metrics dataframe
2024-04-04 12:40:36,197:INFO:Initializing Random Forest Classifier
2024-04-04 12:40:36,197:INFO:Total runtime is 0.046543065706888834 minutes
2024-04-04 12:40:36,199:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:36,199:INFO:Initializing create_model()
2024-04-04 12:40:36,199:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:36,199:INFO:Checking exceptions
2024-04-04 12:40:36,199:INFO:Importing libraries
2024-04-04 12:40:36,199:INFO:Copying training dataset
2024-04-04 12:40:36,201:INFO:Defining folds
2024-04-04 12:40:36,201:INFO:Declaring metric variables
2024-04-04 12:40:36,203:INFO:Importing untrained model
2024-04-04 12:40:36,204:INFO:Random Forest Classifier Imported successfully
2024-04-04 12:40:36,207:INFO:Starting cross validation
2024-04-04 12:40:36,208:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:36,434:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,435:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,440:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,443:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,448:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,455:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,461:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,463:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,464:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,525:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,539:INFO:Calculating mean and std
2024-04-04 12:40:36,539:INFO:Creating metrics dataframe
2024-04-04 12:40:36,540:INFO:Uploading results into container
2024-04-04 12:40:36,540:INFO:Uploading model into container now
2024-04-04 12:40:36,541:INFO:_master_model_container: 7
2024-04-04 12:40:36,541:INFO:_display_container: 2
2024-04-04 12:40:36,541:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-04-04 12:40:36,541:INFO:create_model() successfully completed......................................
2024-04-04 12:40:36,583:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:36,583:INFO:Creating metrics dataframe
2024-04-04 12:40:36,586:INFO:Initializing Quadratic Discriminant Analysis
2024-04-04 12:40:36,587:INFO:Total runtime is 0.053033284346262616 minutes
2024-04-04 12:40:36,588:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:36,588:INFO:Initializing create_model()
2024-04-04 12:40:36,588:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:36,588:INFO:Checking exceptions
2024-04-04 12:40:36,588:INFO:Importing libraries
2024-04-04 12:40:36,589:INFO:Copying training dataset
2024-04-04 12:40:36,590:INFO:Defining folds
2024-04-04 12:40:36,590:INFO:Declaring metric variables
2024-04-04 12:40:36,591:INFO:Importing untrained model
2024-04-04 12:40:36,592:INFO:Quadratic Discriminant Analysis Imported successfully
2024-04-04 12:40:36,595:INFO:Starting cross validation
2024-04-04 12:40:36,596:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:36,621:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-04-04 12:40:36,621:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-04-04 12:40:36,623:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-04-04 12:40:36,624:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-04-04 12:40:36,628:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-04-04 12:40:36,629:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,631:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,632:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,634:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-04-04 12:40:36,635:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-04-04 12:40:36,636:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,641:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,642:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,645:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,646:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-04-04 12:40:36,649:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-04-04 12:40:36,649:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-04-04 12:40:36,654:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,657:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,659:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,674:INFO:Calculating mean and std
2024-04-04 12:40:36,675:INFO:Creating metrics dataframe
2024-04-04 12:40:36,676:INFO:Uploading results into container
2024-04-04 12:40:36,676:INFO:Uploading model into container now
2024-04-04 12:40:36,676:INFO:_master_model_container: 8
2024-04-04 12:40:36,676:INFO:_display_container: 2
2024-04-04 12:40:36,676:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-04-04 12:40:36,676:INFO:create_model() successfully completed......................................
2024-04-04 12:40:36,723:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:36,723:INFO:Creating metrics dataframe
2024-04-04 12:40:36,727:INFO:Initializing Ada Boost Classifier
2024-04-04 12:40:36,727:INFO:Total runtime is 0.055374797185262045 minutes
2024-04-04 12:40:36,729:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:36,729:INFO:Initializing create_model()
2024-04-04 12:40:36,729:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:36,729:INFO:Checking exceptions
2024-04-04 12:40:36,729:INFO:Importing libraries
2024-04-04 12:40:36,729:INFO:Copying training dataset
2024-04-04 12:40:36,731:INFO:Defining folds
2024-04-04 12:40:36,731:INFO:Declaring metric variables
2024-04-04 12:40:36,733:INFO:Importing untrained model
2024-04-04 12:40:36,734:INFO:Ada Boost Classifier Imported successfully
2024-04-04 12:40:36,737:INFO:Starting cross validation
2024-04-04 12:40:36,737:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:36,761:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-04-04 12:40:36,770:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-04-04 12:40:36,773:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-04-04 12:40:36,776:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-04-04 12:40:36,779:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-04-04 12:40:36,787:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-04-04 12:40:36,797:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-04-04 12:40:36,799:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-04-04 12:40:36,805:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-04-04 12:40:36,822:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-04-04 12:40:36,866:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,873:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,874:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,877:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,877:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,881:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,897:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,898:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,901:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,912:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:36,920:INFO:Calculating mean and std
2024-04-04 12:40:36,921:INFO:Creating metrics dataframe
2024-04-04 12:40:36,921:INFO:Uploading results into container
2024-04-04 12:40:36,922:INFO:Uploading model into container now
2024-04-04 12:40:36,922:INFO:_master_model_container: 9
2024-04-04 12:40:36,922:INFO:_display_container: 2
2024-04-04 12:40:36,922:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-04-04 12:40:36,922:INFO:create_model() successfully completed......................................
2024-04-04 12:40:36,964:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:36,964:INFO:Creating metrics dataframe
2024-04-04 12:40:36,968:INFO:Initializing Gradient Boosting Classifier
2024-04-04 12:40:36,968:INFO:Total runtime is 0.05938866138458252 minutes
2024-04-04 12:40:36,969:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:36,970:INFO:Initializing create_model()
2024-04-04 12:40:36,970:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:36,970:INFO:Checking exceptions
2024-04-04 12:40:36,970:INFO:Importing libraries
2024-04-04 12:40:36,970:INFO:Copying training dataset
2024-04-04 12:40:36,972:INFO:Defining folds
2024-04-04 12:40:36,972:INFO:Declaring metric variables
2024-04-04 12:40:36,973:INFO:Importing untrained model
2024-04-04 12:40:36,974:INFO:Gradient Boosting Classifier Imported successfully
2024-04-04 12:40:36,977:INFO:Starting cross validation
2024-04-04 12:40:36,978:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:37,178:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,185:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,189:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,190:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,192:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,201:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,205:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,207:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,217:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,232:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,240:INFO:Calculating mean and std
2024-04-04 12:40:37,241:INFO:Creating metrics dataframe
2024-04-04 12:40:37,242:INFO:Uploading results into container
2024-04-04 12:40:37,242:INFO:Uploading model into container now
2024-04-04 12:40:37,242:INFO:_master_model_container: 10
2024-04-04 12:40:37,242:INFO:_display_container: 2
2024-04-04 12:40:37,243:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-04-04 12:40:37,243:INFO:create_model() successfully completed......................................
2024-04-04 12:40:37,294:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:37,295:INFO:Creating metrics dataframe
2024-04-04 12:40:37,298:INFO:Initializing Linear Discriminant Analysis
2024-04-04 12:40:37,298:INFO:Total runtime is 0.06489554643630982 minutes
2024-04-04 12:40:37,300:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:37,301:INFO:Initializing create_model()
2024-04-04 12:40:37,301:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:37,301:INFO:Checking exceptions
2024-04-04 12:40:37,301:INFO:Importing libraries
2024-04-04 12:40:37,301:INFO:Copying training dataset
2024-04-04 12:40:37,303:INFO:Defining folds
2024-04-04 12:40:37,303:INFO:Declaring metric variables
2024-04-04 12:40:37,305:INFO:Importing untrained model
2024-04-04 12:40:37,306:INFO:Linear Discriminant Analysis Imported successfully
2024-04-04 12:40:37,309:INFO:Starting cross validation
2024-04-04 12:40:37,310:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:37,350:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,353:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,356:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,361:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,364:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,364:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,369:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,373:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,382:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,383:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,388:INFO:Calculating mean and std
2024-04-04 12:40:37,389:INFO:Creating metrics dataframe
2024-04-04 12:40:37,389:INFO:Uploading results into container
2024-04-04 12:40:37,390:INFO:Uploading model into container now
2024-04-04 12:40:37,390:INFO:_master_model_container: 11
2024-04-04 12:40:37,390:INFO:_display_container: 2
2024-04-04 12:40:37,390:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-04-04 12:40:37,390:INFO:create_model() successfully completed......................................
2024-04-04 12:40:37,435:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:37,435:INFO:Creating metrics dataframe
2024-04-04 12:40:37,438:INFO:Initializing Extra Trees Classifier
2024-04-04 12:40:37,439:INFO:Total runtime is 0.06723347902297974 minutes
2024-04-04 12:40:37,440:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:37,440:INFO:Initializing create_model()
2024-04-04 12:40:37,440:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:37,440:INFO:Checking exceptions
2024-04-04 12:40:37,440:INFO:Importing libraries
2024-04-04 12:40:37,440:INFO:Copying training dataset
2024-04-04 12:40:37,442:INFO:Defining folds
2024-04-04 12:40:37,442:INFO:Declaring metric variables
2024-04-04 12:40:37,443:INFO:Importing untrained model
2024-04-04 12:40:37,445:INFO:Extra Trees Classifier Imported successfully
2024-04-04 12:40:37,447:INFO:Starting cross validation
2024-04-04 12:40:37,448:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:37,644:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,644:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,649:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,650:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,652:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,653:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,656:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,661:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,662:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,674:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:37,685:INFO:Calculating mean and std
2024-04-04 12:40:37,686:INFO:Creating metrics dataframe
2024-04-04 12:40:37,687:INFO:Uploading results into container
2024-04-04 12:40:37,687:INFO:Uploading model into container now
2024-04-04 12:40:37,687:INFO:_master_model_container: 12
2024-04-04 12:40:37,688:INFO:_display_container: 2
2024-04-04 12:40:37,688:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-04-04 12:40:37,688:INFO:create_model() successfully completed......................................
2024-04-04 12:40:37,734:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:37,734:INFO:Creating metrics dataframe
2024-04-04 12:40:37,738:INFO:Initializing Light Gradient Boosting Machine
2024-04-04 12:40:37,738:INFO:Total runtime is 0.07222953240076702 minutes
2024-04-04 12:40:37,742:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:37,742:INFO:Initializing create_model()
2024-04-04 12:40:37,742:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:37,742:INFO:Checking exceptions
2024-04-04 12:40:37,743:INFO:Importing libraries
2024-04-04 12:40:37,743:INFO:Copying training dataset
2024-04-04 12:40:37,744:INFO:Defining folds
2024-04-04 12:40:37,744:INFO:Declaring metric variables
2024-04-04 12:40:37,746:INFO:Importing untrained model
2024-04-04 12:40:37,748:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-04 12:40:37,754:INFO:Starting cross validation
2024-04-04 12:40:37,755:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:41,916:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:41,923:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:41,929:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:41,934:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:41,939:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:41,970:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:41,989:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,005:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,009:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,011:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,025:INFO:Calculating mean and std
2024-04-04 12:40:42,026:INFO:Creating metrics dataframe
2024-04-04 12:40:42,027:INFO:Uploading results into container
2024-04-04 12:40:42,027:INFO:Uploading model into container now
2024-04-04 12:40:42,027:INFO:_master_model_container: 13
2024-04-04 12:40:42,027:INFO:_display_container: 2
2024-04-04 12:40:42,028:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-04 12:40:42,028:INFO:create_model() successfully completed......................................
2024-04-04 12:40:42,070:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:42,071:INFO:Creating metrics dataframe
2024-04-04 12:40:42,075:INFO:Initializing Dummy Classifier
2024-04-04 12:40:42,075:INFO:Total runtime is 0.14451236724853517 minutes
2024-04-04 12:40:42,077:INFO:SubProcess create_model() called ==================================
2024-04-04 12:40:42,077:INFO:Initializing create_model()
2024-04-04 12:40:42,077:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x177afc670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:42,077:INFO:Checking exceptions
2024-04-04 12:40:42,077:INFO:Importing libraries
2024-04-04 12:40:42,077:INFO:Copying training dataset
2024-04-04 12:40:42,079:INFO:Defining folds
2024-04-04 12:40:42,079:INFO:Declaring metric variables
2024-04-04 12:40:42,080:INFO:Importing untrained model
2024-04-04 12:40:42,081:INFO:Dummy Classifier Imported successfully
2024-04-04 12:40:42,083:INFO:Starting cross validation
2024-04-04 12:40:42,084:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-04 12:40:42,113:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,115:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:42,116:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,117:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,118:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:42,119:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:42,125:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,125:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,126:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:42,127:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:42,128:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,129:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,130:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:42,131:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:42,136:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,138:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:42,142:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,144:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:42,147:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method='predict_proba', average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/metrics.py", line 188, in _score
    return super()._score(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py", line 248, in transform
    args.append(X[self._include])
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pandas/core/indexes/base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Education'] not in index"

  warnings.warn(

2024-04-04 12:40:42,148:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-04-04 12:40:42,162:INFO:Calculating mean and std
2024-04-04 12:40:42,163:INFO:Creating metrics dataframe
2024-04-04 12:40:42,164:INFO:Uploading results into container
2024-04-04 12:40:42,164:INFO:Uploading model into container now
2024-04-04 12:40:42,164:INFO:_master_model_container: 14
2024-04-04 12:40:42,164:INFO:_display_container: 2
2024-04-04 12:40:42,164:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-04-04 12:40:42,164:INFO:create_model() successfully completed......................................
2024-04-04 12:40:42,204:INFO:SubProcess create_model() end ==================================
2024-04-04 12:40:42,204:INFO:Creating metrics dataframe
2024-04-04 12:40:42,210:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-04-04 12:40:42,213:INFO:Initializing create_model()
2024-04-04 12:40:42,213:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-04 12:40:42,213:INFO:Checking exceptions
2024-04-04 12:40:42,214:INFO:Importing libraries
2024-04-04 12:40:42,214:INFO:Copying training dataset
2024-04-04 12:40:42,216:INFO:Defining folds
2024-04-04 12:40:42,216:INFO:Declaring metric variables
2024-04-04 12:40:42,216:INFO:Importing untrained model
2024-04-04 12:40:42,216:INFO:Declaring custom model
2024-04-04 12:40:42,216:INFO:Random Forest Classifier Imported successfully
2024-04-04 12:40:42,216:INFO:Cross validation set to False
2024-04-04 12:40:42,216:INFO:Fitting Model
2024-04-04 12:40:42,316:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-04-04 12:40:42,316:INFO:create_model() successfully completed......................................
2024-04-04 12:40:42,366:INFO:_master_model_container: 14
2024-04-04 12:40:42,366:INFO:_display_container: 2
2024-04-04 12:40:42,367:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-04-04 12:40:42,367:INFO:compare_models() successfully completed......................................
2024-04-04 12:40:42,383:INFO:Initializing evaluate_model()
2024-04-04 12:40:42,384:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-04-04 12:40:42,390:INFO:Initializing plot_model()
2024-04-04 12:40:42,390:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, system=True)
2024-04-04 12:40:42,390:INFO:Checking exceptions
2024-04-04 12:40:42,406:INFO:Preloading libraries
2024-04-04 12:40:42,410:INFO:Copying training dataset
2024-04-04 12:40:42,410:INFO:Plot type: pipeline
2024-04-04 12:40:42,521:INFO:Visual Rendered Successfully
2024-04-04 12:40:42,578:INFO:plot_model() successfully completed......................................
2024-04-04 12:40:42,696:INFO:Initializing plot_model()
2024-04-04 12:40:42,696:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, system=True)
2024-04-04 12:40:42,696:INFO:Checking exceptions
2024-04-04 12:40:42,731:INFO:Preloading libraries
2024-04-04 12:40:42,735:INFO:Copying training dataset
2024-04-04 12:40:42,735:INFO:Plot type: auc
2024-04-04 12:40:42,796:INFO:Fitting Model
2024-04-04 12:40:42,797:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names
  warnings.warn(

2024-04-04 12:40:42,797:INFO:Scoring test/hold-out set
2024-04-04 12:40:42,912:INFO:Visual Rendered Successfully
2024-04-04 12:40:42,954:INFO:plot_model() successfully completed......................................
2024-04-04 12:40:42,963:INFO:Initializing plot_model()
2024-04-04 12:40:42,963:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, system=True)
2024-04-04 12:40:42,964:INFO:Checking exceptions
2024-04-04 12:40:42,980:INFO:Preloading libraries
2024-04-04 12:40:42,985:INFO:Copying training dataset
2024-04-04 12:40:42,985:INFO:Plot type: confusion_matrix
2024-04-04 12:40:43,043:INFO:Fitting Model
2024-04-04 12:40:43,043:WARNING:/opt/homebrew/Caskroom/miniforge/base/envs/proyectoetica/lib/python3.9/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names
  warnings.warn(

2024-04-04 12:40:43,043:INFO:Scoring test/hold-out set
2024-04-04 12:40:43,128:INFO:Visual Rendered Successfully
2024-04-04 12:40:43,170:INFO:plot_model() successfully completed......................................
2024-04-04 12:40:43,180:INFO:Initializing predict_model()
2024-04-04 12:40:43,180:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2941e34f0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x294460940>)
2024-04-04 12:40:43,180:INFO:Checking exceptions
2024-04-04 12:40:43,180:INFO:Preloading libraries
2024-04-04 12:40:43,181:INFO:Set up data.
2024-04-04 12:40:43,185:INFO:Set up index.
2024-04-04 12:40:43,302:INFO:Initializing save_model()
2024-04-04 12:40:43,302:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), model_name=RandomForestClassifier, prep_pipe_=Pipeline(memory=FastMemory(location=/var/folders/vj/xp36g_9s7lj2v05vvcsms4500000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Income', 'CCAvg', 'Mortgage'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',...
                 TransformerWrapper(exclude=None, include=['Education'],
                                    transformer=OneHotEncoder(cols=['Education'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-04-04 12:40:43,303:INFO:Adding model into prep_pipe
2024-04-04 12:40:43,323:INFO:RandomForestClassifier.pkl saved in current working directory
2024-04-04 12:40:43,331:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Income', 'CCAvg', 'Mortgage'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Education', 'C...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=123, verbose=0,
                                        warm_start=False))],
         verbose=False)
2024-04-04 12:40:43,331:INFO:save_model() successfully completed......................................
